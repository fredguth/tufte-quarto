<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.215">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>The emergence of
an Information Bottleneck Theory
of Deep Learning
- 4&nbsp; Introduction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../Front/ack.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../site_libs/quarto-contrib/quarto-project/tufte/styles.css">
</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introduction</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">The emergence of<br>
an Information Bottleneck Theory<br>
of Deep Learning<br>
</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Front/copyright.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">copyright.html</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Front/dedication.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Dedication</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Front/ack.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/context.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec:greeks" id="toc-sec:greeks" class="nav-link active" data-scroll-target="#sec\:greeks"><span class="toc-section-number">4.0.1</span>  A Tale of Babylonians and Greeks</a></li>
  <li><a href="#the-importance-of-theoretical-narratives" id="toc-the-importance-of-theoretical-narratives" class="nav-link" data-scroll-target="#the-importance-of-theoretical-narratives"><span class="toc-section-number">4.0.2</span>  The importance of theoretical narratives</a></li>
  <li><a href="#sec:bringing_science" id="toc-sec:bringing_science" class="nav-link" data-scroll-target="#sec\:bringing_science"><span class="toc-section-number">4.0.3</span>  Bringing science to Computer Science</a></li>
  <li><a href="#problem" id="toc-problem" class="nav-link" data-scroll-target="#problem"><span class="toc-section-number">4.1</span>  Problem</a>
  <ul class="collapse">
  <li><a href="#possible-new-explanation-in-the-horizon" id="toc-possible-new-explanation-in-the-horizon" class="nav-link" data-scroll-target="#possible-new-explanation-in-the-horizon"><span class="toc-section-number">4.1.1</span>  Possible new explanation in the horizon</a></li>
  <li><a href="#problem-statement" id="toc-problem-statement" class="nav-link" data-scroll-target="#problem-statement"><span class="toc-section-number">4.1.2</span>  Problem statement</a></li>
  </ul></li>
  <li><a href="#objective" id="toc-objective" class="nav-link" data-scroll-target="#objective"><span class="toc-section-number">4.2</span>  Objective</a>
  <ul class="collapse">
  <li><a href="#research-questions" id="toc-research-questions" class="nav-link" data-scroll-target="#research-questions"><span class="toc-section-number">4.2.1</span>  Research Questions</a></li>
  </ul></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology"><span class="toc-section-number">4.3</span>  Methodology</a></li>
  <li><a href="#contributions" id="toc-contributions" class="nav-link" data-scroll-target="#contributions"><span class="toc-section-number">4.4</span>  Contributions</a></li>
  <li><a href="#dissertation-preview-and-outline" id="toc-dissertation-preview-and-outline" class="nav-link" data-scroll-target="#dissertation-preview-and-outline"><span class="toc-section-number">4.5</span>  Dissertation preview and outline</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full column-body" id="quarto-document-content">

<header id="title-block-header">
<h1 class="title display-7"><span id="ch:introduction" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introduction</span></span></h1>

</header>

<div class="page-columns page-full"><p>In his acceptance speech for the Test-of-Time award in NeurIPS 2017,<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Ali Rahimi<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> started a controversy by frankly declaring <span class="citation" data-cites="rahimi:2017">(<a href="#ref-rahimi:2017" role="doc-biblioref">Rahimi 2018, 12</a>’10”)</span>. His concerns on the lack of theoretical understanding of machine learning for critical decision-making are rightful: &gt; | ‘We are building systems that govern healthcare and mediate our civic dialogue. We would influence elections. I would like to live in a society whose systems are built on top of verifiable, rigorous, thorough knowledge and not on alchemy.’</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;Conference on Neural Information Processing.</p></li><li id="fn2"><p><sup>2</sup>&nbsp;Research Scientist, Google.</p></li><div id="ref-rahimi:2017" class="csl-entry" role="doc-biblioentry">
Rahimi, Ali. 2018. <span>“Ali Rahimi NIPS 2017 Test-of-Time Award Presentation Speech.”</span> <a href="https://youtu.be/x7psGHgatGM" class="uri">https://youtu.be/x7psGHgatGM</a>; Youtube. <a href="https://youtu.be/x7psGHgatGM">https://youtu.be/x7psGHgatGM</a>.
</div></div></div>
<div class="page-columns page-full"><p>The next day, Yann LeCun<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> responded: &gt; | ‘Criticising an entire com- munity (. . .) for practising “alchemy”, simply because our current theoretical tools have not caught up with our practice is dangerous.’</p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;Deep Learning pioneer and 2018 Turing award winner. <a href="https://www.facebook.com/yann.lecun/posts/10154938130592143" class="uri">https://www.facebook.com/yann.lecun/posts/10154938130592143</a></p></li></div></div>
<p>Both researchers, at least, agree upon one thing: the practice of machine learning has outpaced its theoretical development. That is certainly a research opportunity.</p>
<section id="sec:greeks" class="level3 page-columns page-full" data-number="4.0.1">
<h3 data-number="4.0.1" class="anchored" data-anchor-id="sec:greeks"><span class="header-section-number">4.0.1</span> A Tale of Babylonians and Greeks</h3>
<!-- Q: How to solve marginnote already defined? -->
<!-- ![Richard Feynman, Nobel laureate physicist.](feynman.pdf){.column-margin #fig-feynman} -->
<p>Richard Feynman (<a href="#fig:feynman" data-reference-type="ref" data-reference="fig:feynman">[fig:feynman]</a>) used to lecture this story&nbsp;<span class="citation" data-cites="feynman:1994">(<a href="#ref-feynman:1994" role="doc-biblioref">Feynman 1994</a>)</span>: Babylonians were pioneers in mathematics; Yet, the Greeks took the credit. We are used to the Greek way of doing Math: start from the most basic axioms and build up a knowledge system. Babylonians were quite the opposite; they were pragmatic. No knowledge was considered more fundamental than others, and there was no urge to derive proofs in a particular order. Babylonians were concerned with the phenomena, Greeks with the ordinance. In Feynman’s view, science is constructed in the Babylonian way. There is no fundamental truth. Theories try to connect dots from different pieces of knowledge. Only as science advances, one can worry about reformulation, simplification and ordering. Scientists are Babylonians; mathematicians are Greeks.</p>
<div class="no-row-height column-margin column-container"><div id="ref-feynman:1994" class="csl-entry" role="doc-biblioentry">
Feynman, Richard. 1994. <em>The Character of Physical Law</em>. Modern Library.
</div></div><p>Mathematics and science are both tools for knowledge acquisition. They are also social constructs that rely on peer-reviewing. They are somewhat different, however.</p>
<p>Science is empiric, based on facts collected from <strong>experience</strong>. When physicists around the world measured events that corroborated Newton’s <em>“Law of Universal Gravitation”</em>, they did not prove it correct; they just made his theory more and more plausible. Still, only one experiment was needed to show that Einstein’s <em>Relativity Theory</em> was even more believable. In contrast, we can and do prove things in mathematics.</p>
<p>In mathematics, knowledge is absolute truth, and the way one builds new knowledge with it, its inference method, is deduction. Mathematics is a language, a formal one, a tool to precisely communicate some kinds of thoughts. As it happens with natural languages, there is beauty in it. The mathematician expands the boundaries of expression in this language.</p>
<p>In science, there are no axioms: a falsifiable hypothesis/theory is proposed, and logical conclusions (predictions) from the theory are empirically tested. Despite inferring hypotheses by induction, there is no influence of psychology in the process. A tested hypothesis is not absolute truth. A hypothesis is never verified, only falsified by experiments&nbsp;<span class="citation" data-cites="popper:2004">(<a href="#ref-popper:2004" role="doc-biblioref">Popper 2004, 31–50</a>)</span>. Scientific knowledge is belief justified by experience; there are degrees of plausibility.</p>
<div class="no-row-height column-margin column-container"><div id="ref-popper:2004" class="csl-entry" role="doc-biblioentry">
Popper, Karl. 2004. <em>A Lógica Da Pesquisa Científica</em>. Translated by Leonidas Hegenberg and Octanny Silveira. São Paulo: Cultrix.
</div></div><p>Understanding the epistemic contrast between mathematics and science will help us understand the past of <span data-acronym-label="AI" data-acronym-form="singular+full">AI</span> and avoid some perils in its future.</p>
</section>
<section id="the-importance-of-theoretical-narratives" class="level3 page-columns page-full" data-number="4.0.2">
<h3 data-number="4.0.2" class="anchored" data-anchor-id="the-importance-of-theoretical-narratives"><span class="header-section-number">4.0.2</span> The importance of theoretical narratives</h3>
<p><strong>Science is a narrative</strong> of how we understand Nature&nbsp;<span class="citation" data-cites="gleiser:2018">(<a href="#ref-gleiser:2018" role="doc-biblioref">Gleiser and Sowinski 2018</a>)</span>. In science, we collect facts, but they need interpretation. The logical conclusion from the hypothesis that predicts some behaviour in nature gives a plausible <em>meaning</em> to what we observed.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gleiser:2018" class="csl-entry" role="doc-biblioentry">
Gleiser, Marcelo, and Damian Sowinski. 2018. <span>“The Map and the Territory.”</span> In <em>The Frontiers Collection</em>, edited by Shyam Wuppuluri and Francisco Antonio Doria. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-72478-2">https://doi.org/10.1007/978-3-319-72478-2</a>.
</div><div id="ref-farrington:2016" class="csl-entry" role="doc-biblioentry">
Farrington, Karen. 2016. <em>The Blitzed City: The Destruction of Coventry, 1940</em>. London: Aurum Press.
</div></div><p>To illustrate, take the ancient human desire of flying. There have always been stories of men strapping wings to themselves and attempting to fly by jumping from a tower and flapping those wings like birds <span class="citation" data-cites="farrington:2016">(see <a href="#ref-farrington:2016" role="doc-biblioref">Farrington 2016</a>)</span>. While concepts like lift, stability, and control were poorly understood, most human flight attempts ended in severe injury or even death. It did not matter how much evidence, how many hours of seeing different animals flying, those ludicrous brave men experienced; the <em>meaning</em> they took from what they saw was wrong, and their predictions incorrect.</p>
<div class="page-columns page-full"><p>They did not die in vain<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>; Science advances when scientists are wrong. Theories must be falsifiable, and scientists cheer for their failure. When it fails, there is room for new approaches. Only when we understood the observations in animal flight from the aerodynamics perspective, we learned to fly better than any other animal before. Science works by a “natural selection” of ideas, where only the fittest ones survive until a better one is born. Chaitin also points out that an idea has “fertility” to the extent to which it “illuminates us, inspires us with other ideas, and suggests unsuspected connections and new viewpoints”&nbsp;<span class="citation" data-cites="chaitin:2006">(<a href="#ref-chaitin:2006" role="doc-biblioref">Chaitin 2006, 9</a>)</span>.</p><div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;Those “researchers” deserved, at least, a Darwin Award of Science. The Darwin Award is satirical honours that recognise individuals who have unwillingly contributed to human evolution by selecting themselves out of the gene pool.</p></li><div id="ref-chaitin:2006" class="csl-entry" role="doc-biblioentry">
Chaitin, Gregory. 2006. <em>Meta Math! The Quest for Omega</em>. Vintage Books.
</div></div></div>
<p>Being a Babylonian enterprise, science has no clear path. One of the exciting facts one can learn by studying its history is that robust discoveries have arisen through the study of phenomena in human-made devices&nbsp;<span class="citation" data-cites="pierce:1980">(<a href="#ref-pierce:1980" role="doc-biblioref">Pierce, n.d.</a>)</span>. For instance, Carnot’s first and only scientific work&nbsp;<span class="citation" data-cites="klein:1974">(<a href="#ref-klein:1974" role="doc-biblioref">Klein 1974</a>)</span> gave birth to thermodynamics: the study of energy, the conversion between its different forms, and the ability of energy to do work;&nbsp;the science that explains how steam engines work. However, steam engines came before Carnot’s work and were studied by him. Such human-made devices may present a simplified instance of more complex natural phenomena.</p>
<div class="no-row-height column-margin column-container"><div id="ref-pierce:1980" class="csl-entry" role="doc-biblioentry">
Pierce, John R. n.d. <em>An Introduction to Information Theory: Symbols, Signals and Noise</em>. Dover Publications.
</div><div id="ref-klein:1974" class="csl-entry" role="doc-biblioentry">
Klein, Martin J. 1974. <span>“Carnot<span></span>s Contribution to Thermodynamics.”</span> <em>Physics Today</em> 27 (8): 23–28. <a href="https://doi.org/10.1063/1.3128802">https://doi.org/10.1063/1.3128802</a>.
</div><div id="ref-shannon:1948" class="csl-entry" role="doc-biblioentry">
Shannon, Claude E. 1948. <span>“A Mathematical Theory of Communication.”</span> <em>Bell System Technical Journal</em> 27 (3): 379–423.
</div></div><div class="page-columns page-full"><p>Another example is Information Theory. Several insights of Shannon’s theory of communication were generalisations of ideas already present in Telegraphy&nbsp;<span class="citation" data-cites="shannon:1948">(<a href="#ref-shannon:1948" role="doc-biblioref">Shannon 1948</a>)</span>. New theories in artificial intelligence can, therefore, be developed from insights in the study of deep learning phenomena.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;Understanding human intelligence using artificial intelligence is a field of study called Computational Neuroscience.</p></li></div></div>
</section>
<section id="sec:bringing_science" class="level3 page-columns page-full" data-number="4.0.3">
<h3 data-number="4.0.3" class="anchored" data-anchor-id="sec:bringing_science"><span class="header-section-number">4.0.3</span> Bringing science to Computer Science</h3>
<p>Despite the name, Computer Science has been more mathematics than science. We, computer scientists, are very comfortable with theorems and proofs, not much with theories.</p>
<p>Nevertheless, <span data-acronym-label="AI" data-acronym-form="singular+short">AI</span> has essentially become a Babylonian enterprise, a scientific endeavour. Thus, there is no surprise when some computer scientists still see AI with some distrust and even disdain, despite its undeniable usefulness:</p>
<ul>
<li><p>Even among AI researchers, there is a trend of “mathiness” and speculation disguised as explanations in conference papers&nbsp;<span class="citation" data-cites="lipton:2018">(<a href="#ref-lipton:2018" role="doc-biblioref">Lipton and Steinhardt 2018</a>)</span>.</p></li>
<li><p>There are few venues for papers that describe surprising phenomena without trying to come up with an explanation. As if the mere inconsistency of the current theoretical framework was unworthy of publication.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-lipton:2018" class="csl-entry" role="doc-biblioentry">
Lipton, Zachary C., and Jacob Steinhardt. 2018. <span>“Troubling Trends in Machine Learning Scholarship.”</span> <a href="https://arxiv.org/abs/1807.03341">https://arxiv.org/abs/1807.03341</a>.
</div></div><div class="page-columns page-full"><p>While physicists rejoice in finding phenomena that contradict current theories, computer scientists get baffled. In Natural Sciences, unexplained phenomena lead to theoretical development. Some believe they bring <em>winters</em>, periods of progress stagnation and lack of funding in <span data-acronym-label="AI" data-acronym-form="singular+short">AI</span>.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;This seems to be Yann LeCun’s opinion: However, due to all possible alternative explanations (lack of computational power, no availability of massive annotated datasets), it seems harsh or simply wrong to blame theorists.</p></li></div></div>
<div class="page-columns page-full"><p>Artificial Intelligence has been through several of the aforementioned “winters”. In 1957, Herbert Simon<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> famously predicted that within ten years, a computer would be a chess champion&nbsp;<span class="citation" data-cites="russell:2010">(<a href="#ref-russell:2010" role="doc-biblioref">Russell, Norvig, and Davis 2010, sec. 1.3</a>)</span>. It took around 40 years, in any case. Computer scientists lacked understanding of the exponential nature of the problems they were trying to solve: Computational Complexity Theory had yet to be invented.</p><div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;Herbert Simon (<span class="math inline">\(1916\)</span>–<span class="math inline">\(2001\)</span>) received the Turing Award in <span class="math inline">\(1975\)</span>, and the Nobel Prize in Economics in <span class="math inline">\(1978\)</span>.</p></li><div id="ref-russell:2010" class="csl-entry" role="doc-biblioentry">
Russell, Stuart J., Peter Norvig, and Ernest Davis. 2010. <em>Artificial Intelligence: A Modern Approach</em>. 3rd ed. Prentice <span><span>Hall</span> </span> Series in Artificial Intelligence. Prentice Hall.
</div></div></div>
<p>Machine Learning Theory (computational and statistical) tries to avoid a similar trap by analysing and classifying learning problems according to the number of samples required to learn them (besides the number of steps). The matter of concern is that it currently predicts that generalisation requires simpler models in terms of parameters. In total disregard to the theory, deep learning models have shown spectacular generalisation power with hundreds of millions of parameters (and even more impressive overfitting capacity&nbsp;).</p>
</section>
<section id="problem" class="level2 page-columns page-full" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="problem"><span class="header-section-number">4.1</span> Problem</h2>
<p>In the last decade, we have witnessed a myriad of astonishing successes in Deep Learning. Despite those many successes in research and industry applications, we may again be climbing a peak of inflated expectations. If in the past, the false solution was to “add computation power” on problems, today we try to solve them by “piling data”(<a href="#fig:machine_learning_2x" data-reference-type="ref" data-reference="fig:machine_learning_2x">[fig:machine_learning_2x]</a>). Such behaviour has triggered a winner-takes-all competition for who collects more data (our data) amidst a handful of large corporations, raising ethical concerns about privacy and concentration of power&nbsp;<span class="citation" data-cites="oneil:2016">(<a href="#ref-oneil:2016" role="doc-biblioref">O’Neil 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-oneil:2016" class="csl-entry" role="doc-biblioentry">
O’Neil, Cathy. 2016. <em>Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy</em>. USA: Crown Publishing Group.
</div><div id="ref-zhang:2016" class="csl-entry" role="doc-biblioentry">
Zhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 2016. <span>“Understanding Deep Learning Requires Rethinking Generalization.”</span> <a href="https://arxiv.org/abs/1611.03530">https://arxiv.org/abs/1611.03530</a>.
</div></div><p>Nevertheless, we know that learning from way fewer samples is possible: humans show a much better generalisation ability than our current state-of-the-art artificial intelligence. To achieve such needed generalisation power, we may need to understand better how learning happens in deep learning. Rethinking generalisation might reshape the foundations of machine learning theory&nbsp;<span class="citation" data-cites="zhang:2016">(<a href="#ref-zhang:2016" role="doc-biblioref">Zhang et al. 2016</a>)</span>.</p>
<section id="possible-new-explanation-in-the-horizon" class="level3 page-columns page-full" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="possible-new-explanation-in-the-horizon"><span class="header-section-number">4.1.1</span> Possible new explanation in the horizon</h3>
<div class="page-columns page-full"><p>In <span class="math inline">\(2015\)</span>, &nbsp;<span class="citation" data-cites="tishby:2015dlib">Tishby and Zaslavsky (<a href="#ref-tishby:2015dlib" role="doc-biblioref">2015</a>)</span> proposed a theory of deep learning &nbsp;<span class="citation" data-cites="tishby:2015dlib">(<a href="#ref-tishby:2015dlib" role="doc-biblioref">Tishby and Zaslavsky 2015</a>)</span> based on the information-theoretical concept of the bottleneck principle, of which Tishby is one of the authors. Later, in 2017, &nbsp;<span class="citation" data-cites="shwartz-ziv:2017">Shwartz-Ziv and Tishby (<a href="#ref-shwartz-ziv:2017" role="doc-biblioref">2017</a>)</span> followed up on the <span data-acronym-label="IBT" data-acronym-form="singular+full">IBT</span> with the paper &nbsp;, which was presented in a well-attended workshop<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, with appealing visuals that clearly showed a <em>“phase transition”</em> happening during training. The video posted on Youtube&nbsp;<span class="citation" data-cites="tishby:2017yt1">(<a href="#ref-tishby:2017yt1" role="doc-biblioref">Tishby 2017</a>)</span> became a “sensation”<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, and received a wealth of publicity when well-known researchers like Geoffrey Hinton<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, Samy Bengio (Apple) and Alex Alemi (Google Research) have expressed interest in Tishby’s ideas&nbsp;<span class="citation" data-cites="wolchover:2017quanta">(<a href="#ref-wolchover:2017quanta" role="doc-biblioref">Wolchover 2017</a>)</span>. they are called formal languages.</p><div class="no-row-height column-margin column-container"><li id="fn8"><p><sup>8</sup>&nbsp;Deep Learning: Theory, Algorithms, and Applications. Berlin, June 2017 <a href="http://doc.ml.tu-berlin.de/dlworkshop2017" class="uri">http://doc.ml.tu-berlin.de/dlworkshop2017</a></p></li><div id="ref-tishby:2017yt1" class="csl-entry" role="doc-biblioentry">
Tishby, Naftali. 2017. <span>“Information Theory of Deep Learning.”</span> <a href="https://youtu.be/bLqJHjXihK8" class="uri">https://youtu.be/bLqJHjXihK8</a>. <a href="https://youtu.be/bLqJHjXihK8">https://youtu.be/bLqJHjXihK8</a>.
</div><li id="fn9"><p><sup>9</sup>&nbsp;By the time of this writing, this video as more than <span class="math inline">\(84,000\)</span> views, which is remarkable for an hour-long workshop presentation in an academic niche. <a href="https://youtu.be/bLqJHjXihK8" class="uri">https://youtu.be/bLqJHjXihK8</a></p></li><li id="fn10"><p><sup>10</sup>&nbsp;Another Deep Learning Pioneer <strong>and</strong> Turing award winner (2018).</p></li><div id="ref-wolchover:2017quanta" class="csl-entry" role="doc-biblioentry">
Wolchover, Natalie. 2017. <span>“New Theory Cracks Open the Black Box of Deep Learning.”</span> <a href="https://www.quantamagazine.org" class="uri">https://www.quantamagazine.org</a>/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/; Simons Foundation.
</div></div></div>
<div class="no-row-height column-margin column-container"><div id="ref-tishby:2015dlib" class="csl-entry" role="doc-biblioentry">
Tishby, Naftali, and Noga Zaslavsky. 2015. <span>“Deep Learning and the Information Bottleneck Principle.”</span> In <em>2015 IEEE Information Theory Workshop (ITW)</em>, 1–5. IEEE.
</div></div><blockquote class="blockquote">
<p><em>I believe that the information bottleneck idea could be very important in future deep neural network research.</em> — Alex Alemi</p>
</blockquote>
<p>Andrew Saxe (Harvard University) rebutted &nbsp;<span class="citation" data-cites="shwartz-ziv:2017">Shwartz-Ziv and Tishby (<a href="#ref-shwartz-ziv:2017" role="doc-biblioref">2017</a>)</span> claims in &nbsp; and was followed by other critics. According to Saxe, it was impossible to reproduce &nbsp;<span class="citation" data-cites="shwartz-ziv:2017">(<a href="#ref-shwartz-ziv:2017" role="doc-biblioref">Shwartz-Ziv and Tishby 2017</a>)</span>’s experiments with different parameters.</p>
<div class="no-row-height column-margin column-container"><div id="ref-shwartz-ziv:2017" class="csl-entry" role="doc-biblioentry">
Shwartz-Ziv, Ravid, and Naftali Tishby. 2017. <span>“Opening the Black Box of Deep Neural Networks via Information.”</span> <a href="https://arxiv.org/abs/1703.00810">https://arxiv.org/abs/1703.00810</a>.
</div></div><p><em>Has the initial enthusiasm on the <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> been unfounded? Have we let us “fool ourselves” by beautiful charts and a good story?</em></p>
</section>
<section id="problem-statement" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="problem-statement"><span class="header-section-number">4.1.2</span> Problem statement</h3>
<p><strong>The practice of modern machine learning has outpaced its theoretical development.</strong> In particular, deep learning models present generalisation capabilities unpredicted by the current machine learning theory. There is yet no established new general theory of learning which handles this problem.</p>
<p><span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> was proposed as a possible new theory with the <strong>potential</strong> of filling the theory-practice gap. Unfortunately, to the extent of our knowledge, <strong>there is still no comprehensive digest of <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> nor an analysis of how it relates to current <span data-acronym-label="MLT" data-acronym-form="singular+full">MLT</span></strong>.</p>
</section>
</section>
<section id="objective" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="objective"><span class="header-section-number">4.2</span> Objective</h2>
<p>This dissertation aims to investigate <em>to what extent</em> can the emergent Information Bottleneck Theory help us better understand Deep Learning and its phenomena, especially generalisation, presenting its strengths, weaknesses and research opportunities.</p>
<section id="research-questions" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="research-questions"><span class="header-section-number">4.2.1</span> Research Questions</h3>
</section>
</section>
<section id="methodology" class="level2 page-columns page-full" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="methodology"><span class="header-section-number">4.3</span> Methodology</h2>
<ol type="1">
<li><p>Given that <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> is yet not a well-established learning theory, there were two difficulties that the research had to address:</p>
<ol type="1">
<li><p>There is a growing interest in the subject, and new papers are published every day. It was essential to select literature and restrain the analysis.</p></li>
<li><p>Early on, the marks of an emergent theory in its infancy manifested in the form of missing assumptions, inconsistent notation, borrowed jargon, and seeming missing steps. Foremost, it was unclear what was missing from the theory and what was missing in our understanding.</p></li>
</ol>
<p>An initial literature review on <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> was conducted to define the scope.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> We then chose to narrow the research to <strong>theoretical perspective on generalisation</strong>, where we considered that it could bring fundamental advances. We made the deliberate choice of going deeper in a limited area of <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> and not broad, leaving out a deeper experimental and application analysis, all the work on <span data-acronym-label="ITL" data-acronym-form="singular+full">ITL</span><a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> &nbsp;<span class="citation" data-cites="principe:2010">(<a href="#ref-principe:2010" role="doc-biblioref">Principe 2010</a>)</span> and statistical-mechanics-based analysis of SGD &nbsp;<span class="citation" data-cites="chaudhari:2018SGD chaudhari:2019">(<a href="#ref-chaudhari:2018SGD" role="doc-biblioref">P. Chaudhari and Soatto 2018</a>; <a href="#ref-chaudhari:2019" role="doc-biblioref">Pratik Chaudhari et al. 2019</a>)</span>. From this set of constraints, we chose a list of pieces of <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> literature to go deeper (<a href="#ch:literature" data-reference-type="ref" data-reference="ch:literature">[ch:literature]</a>).</p></li>
<li><p>In order to answer , we discuss the epistemology of <span data-acronym-label="AI" data-acronym-form="singular+long">AI</span> to choose fundamental axioms (definition of intelligence and the definition of knowledge) with which we deduced from the ground up <span data-acronym-label="MLT" data-acronym-form="singular+short">MLT</span>, <span data-acronym-label="IT" data-acronym-form="singular+short">IT</span> and <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span>, revealing hidden assumptions, pointing out similarities and differences. By doing that, we built a “genealogy” of these research fields. This comparative study was essential for identifying missing gaps and research opportunities.</p></li>
<li><p>In order to answer , we first dissected the selected literature (<a href="#ch:literature" data-reference-type="ref" data-reference="ch:literature">[ch:literature]</a>) and organised scattered topics in a comprehensive sequence of subjects.</p></li>
<li><p>In the process of the literature digest, we identified results, strengths, weaknesses and research opportunities.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn11"><p><sup>11</sup>&nbsp;Not even the term <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> is universally adopted.</p></li><li id="fn12"><p><sup>12</sup>&nbsp;<span data-acronym-label="ITL" data-acronym-form="singular+short">ITL</span> makes the opposite path we are taking, bringing concepts of machine learning to information theory problems.</p></li><div id="ref-principe:2010" class="csl-entry" role="doc-biblioentry">
Principe, Jose C. 2010. <em>Information Theoretic Learning: Renyi’s Entropy and Kernel Perspectives</em>. Springer Science &amp; Business Media.
</div><div id="ref-chaudhari:2018SGD" class="csl-entry" role="doc-biblioentry">
Chaudhari, P., and S. Soatto. 2018. <span>“Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks.”</span> In <em>2018 Information Theory and Applications Workshop (ITA)</em>, 1–10. <a href="https://doi.org/10.1109/ITA.2018.8503224">https://doi.org/10.1109/ITA.2018.8503224</a>.
</div><div id="ref-chaudhari:2019" class="csl-entry" role="doc-biblioentry">
Chaudhari, Pratik, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. 2019. <span>“Entropy-Sgd: Biasing Gradient Descent into Wide Valleys.”</span> <em>Journal of Statistical Mechanics: Theory and Experiment</em> 2019 (12).
</div></div></section>
<section id="contributions" class="level2 page-columns page-full" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="contributions"><span class="header-section-number">4.4</span> Contributions</h2>
<p>In the research conducted, we produced three main results that, to the extent of our knowledge, are original:</p>
<ol type="1">
<li><p>The dissertation itself is the main expected result: a comprehensive digest of the <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> literature and a snapshot analysis of the field in its current form, focusing on its theoretical implications for generalisation.</p></li>
<li><p>We propose an Information-Theoretical learning problem different from <span data-acronym-label="MDL" data-acronym-form="singular+short">MDL</span> proposed by &nbsp;<span class="citation" data-cites="hinton:1993">(<a href="#ref-hinton:1993" role="doc-biblioref">Hinton and Van Camp 1993</a>)</span> for which we derived bounds using Shannon’s . These results, however, are only indicative as they lack peer review to be validated.</p></li>
<li><p>We present a critique on <span class="citation" data-cites="achille:2019phd">Achille (<a href="#ref-achille:2019phd" role="doc-biblioref">2019</a>)</span>’s explanation&nbsp;<span class="citation" data-cites="achille:2019phd achille:2017emergence">(<a href="#ref-achille:2019phd" role="doc-biblioref">Achille 2019</a>; <a href="#ref-achille:2017emergence" role="doc-biblioref">Achille and Soatto 2018</a>)</span> for the role of layers in Deep Representation in the <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> perspective&nbsp;(<a href="#sec:achille_proof_critique" data-reference-type="ref" data-reference="sec:achille_proof_critique">[sec:achille_proof_critique]</a>), pointing out a weakness in the argument that, as far as we know, has not yet been presented. We then propose a counter-intuitive <em>hypothesis</em> that layers reduce the model’s “effective” hypothesis space. This <em>hypothesis</em> is not formally proven in the present work, but we try to give the intuition behind it (<a href="#sec:proposed_hypothesis" data-reference-type="ref" data-reference="sec:proposed_hypothesis">[sec:proposed_hypothesis]</a>). This result has not yet been validated as well.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-hinton:1993" class="csl-entry" role="doc-biblioentry">
Hinton, Geoffrey E, and Drew Van Camp. 1993. <span>“Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights.”</span> In <em>Proceedings of the Sixth Annual Conference on Computational Learning Theory</em>, 5–13.
</div><div id="ref-achille:2019phd" class="csl-entry" role="doc-biblioentry">
Achille, Alessandro. 2019. <span>“Emergent Properties of Deep Neural Networks.”</span> PhD thesis, UCLA. <a href="https://escholarship.org/uc/item/8gb8x6w9">https://escholarship.org/uc/item/8gb8x6w9</a>.
</div><div id="ref-achille:2017emergence" class="csl-entry" role="doc-biblioentry">
Achille, Alessandro, and Stefano Soatto. 2018. <span>“Emergence of Invariance and Disentangling in Deep Representations.”</span> <em>J. Mach. Learn. Res.</em> 19 (1): 1947–80.
</div></div></section>
<section id="dissertation-preview-and-outline" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="dissertation-preview-and-outline"><span class="header-section-number">4.5</span> Dissertation preview and outline</h2>
<p>The dissertation is divided into two main parts (<a href="#pt:background" data-reference-type="ref" data-reference="pt:background">[pt:background]</a> and <a href="#pt:emergence_of_theory" data-reference-type="ref" data-reference="pt:emergence_of_theory">[pt:emergence_of_theory]</a>), with a break in the middle (<a href="#pt:intermezzo" data-reference-type="ref" data-reference="pt:intermezzo">[pt:intermezzo]</a>).</p>
<ol type="1">
<li><p>Background (<a href="#pt:background" data-reference-type="ref" data-reference="pt:background">[pt:background]</a>)</p>
<ul>
<li><p>Chapter 2–Artificial Intelligence: The chapter defines what artificial intelligence is, presents the epistemological differences of intelligent agents in history, and discusses their consequences to machine learning theory.</p></li>
<li><p>Chapter 3 — Probability Theory: The chapter derives propositional calculus and probability theory from a list of desired characteristics for epistemic agents. It also presents basic Probability Theory concepts.</p></li>
<li><p>Chapter 4 — Machine Learning Theory: The chapter presents the theoretical framework of Machine Learning, the PAC model, theoretical guarantees for generalisation, and expose its weaknesses concerning Deep Learning phenomena.</p></li>
<li><p>Chapter 5 — Information Theory: The chapter derives Shannon Information from Probability Theory, explicates some implicit assumptions, and explains basic Information Theory concepts.</p></li>
</ul></li>
<li><p>Intermezzo (<a href="#pt:intermezzo" data-reference-type="ref" data-reference="pt:intermezzo">[pt:intermezzo]</a>)</p>
<ul>
<li>Chapter 6 — Information-Theoretical Epistemology: This chapter closes the background part and opens the IBT part of the dissertation. It shows the connection of <span data-acronym-label="IT" data-acronym-form="singular+short">IT</span> and <span data-acronym-label="MLT" data-acronym-form="singular+short">MLT</span> in the learning problem, proves that Shannon theorems can be used to prove PAC bounds and present the <span data-acronym-label="MDL" data-acronym-form="singular+full">MDL</span> Principle, an earlier example of this kind of connection.</li>
</ul></li>
<li><p>The emergence of a theory (<a href="#pt:emergence_of_theory" data-reference-type="ref" data-reference="pt:emergence_of_theory">[pt:emergence_of_theory]</a>)</p>
<ul>
<li><p>Chapter 7 — IB Principle: Explains the IB method and its tools: <span data-acronym-label="KL" data-acronym-form="singular+full">KL</span> as a natural distortion (loss) measure, the IB Lagrangian and the Information Plane.</p></li>
<li><p>Chapter 8 — IB and Representation Learning: Presents the learning problem in the <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> perspective (not specific to <span data-acronym-label="DL" data-acronym-form="singular+short">DL</span>). It shows how some usual choices of the practice of <span data-acronym-label="DL" data-acronym-form="singular+short">DL</span> emerge naturally from a list of desired properties of representations. It also shows that the information in the weights bounds the information in the activations.</p></li>
<li><p>Chapter 9 — IB and Deep Learning: This chapter presents the <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> perspective specific to Deep Learning. It presents <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> analysis of Deep Learning training, some examples of applications of <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> to improve or create algorithms; and the <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> learning theory of Deep Learning. We also explain Deep Learning phenomena in the <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> perspective.</p></li>
<li><p>Chapter 10 — Conclusion: In this chapter, we present a summary of the findings, answer the research questions, and present suggestions for future work.</p></li>
</ul></li>
</ol>
<p>We found out that <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> does not invalidate <span data-acronym-label="MLT" data-acronym-form="singular+short">MLT</span>; it just interprets complexity not as a function of the data (number of parameters) but as a function of the information contained in the data. With this interpretation, there is no paradox in improving generalisation by adding layers.</p>
<p>Furthermore, they both share more or less the same “genealogy” of assumptions. <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> can be seen as particular case of <span data-acronym-label="MLT" data-acronym-form="singular+short">MLT</span>. Nevertheless, <span data-acronym-label="IBT" data-acronym-form="singular+short">IBT</span> allows us to better understand the training process and provide a different narrative that helps us comprehend Deep Learning phenomena in a more general way.</p>



</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../Front/ack.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Acknowledgements</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>